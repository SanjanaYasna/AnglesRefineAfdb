{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "from torch.utils.data import SubsetRandomSampler, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from Model import Transformer\n",
    "import time\n",
    "import os\n",
    "from PDB2Angles import extract_backbone_model\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from operator import itemgetter\n",
    "from itertools import groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_Longtensor(data):\n",
    "    data = torch.LongTensor(data)\n",
    "    return data\n",
    "\n",
    "def make_decoderinput(data):\n",
    "    for i in range(len(data)):\n",
    "        data[i].insert(0, 1001)\n",
    "        # data[i].append(36002)\n",
    "    return data\n",
    "\n",
    "def recover_target(data):\n",
    "    for i in range(len(data)):\n",
    "        del data[i][0]\n",
    "    return data\n",
    "\n",
    "def make_decoderoutput(data):\n",
    "    for i in range(len(data)):\n",
    "        data[i].append(1002)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_convertion(data, maxAngle, minAngle):\n",
    "    normlist = []\n",
    "    for val in data:\n",
    "        val = float(val)\n",
    "        normVal = (val - minAngle) / (maxAngle - minAngle)\n",
    "        normlist.append(round(normVal, 3))\n",
    "\n",
    "    normlist = [i * 1000 for i in normlist]\n",
    "    return normlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset inputs:\n",
    "#SS (helix, sheet, coil)\n",
    "#phi, psi, omega, CCN, CNC, NCC angle\n",
    "from typing import Any\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class GetAngleForSSData(Data.Dataset):\n",
    "    def __init__(self, data_path, ss, angle, batch_size = 4):\n",
    "        self.data_path = data_path\n",
    "        self.ss = ss\n",
    "        self.angle = angle\n",
    "        self.proteins = os.listdir(data_path)[:30]\n",
    "    def __len__(self):\n",
    "        return len(self.proteins)\n",
    "    \n",
    "    #return source and target angles list (these values are in the range of [0, 1000] after being normalized from 0 to 1 (in terms of min and max angle range) * 1000)\n",
    "    def __getitem__(self, index) -> Any:\n",
    "        protein = self.proteins[index]\n",
    "        # #enc angle and inflate values for training convenience \n",
    "        # train_enc_inputs = convert_to_Longtensor(source_angle)\n",
    "        target = pd.read_csv(os.path.join(self.data_path, protein,\"target.csv\"), usecols=[\"SS\", self.angle])\n",
    "        target_indices = target[target[\"SS\"] == self.ss].index\n",
    "        #put indices in gropus of consecutive indices\n",
    "        target_angle = target[target[\"SS\"] == self.ss][self.angle].values \n",
    "        #find the corresponding indices in target.csv that have ss equal to desired self.ss\n",
    "        #get the corresponding source angle values\n",
    "        source = pd.read_csv(os.path.join(self.data_path, protein,\"source.csv\"), usecols=[ self.angle])\n",
    "        source_angle = source.iloc[target_indices][self.angle].values \n",
    "        # train_dec_inputs = convert_to_Longtensor(make_decoderinput(target_angle))\n",
    "        # train_dec_outputs = convert_to_Longtensor(make_decoderoutput(recover_target(target_angle)))\n",
    "        if self.angle == \"omega\" or self.angle == \"psi_im1\" or self.angle == \"phi\":\n",
    "            target_angle = data_convertion(target_angle, 180.0, -180.0)\n",
    "            source_angle = data_convertion(source_angle, 180.0, -180.0)\n",
    "        else:\n",
    "            target_angle = data_convertion(target_angle, 180.0, 0.0)\n",
    "            source_angle = data_convertion(source_angle, 180.0, 0.0)\n",
    "        return source_angle, target_angle#train_dec_inputs, train_dec_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = GetAngleForSSData(\"/kuhpc/work/slusky/s300y051/AnglesRefine/example/sample_source_targets\", \"H\", \"phi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next a batch from trainloader\n",
    "trainloader = DataLoader(data, batch_size=4, shuffle=True, collate_fn=collate_fn_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_padded(batch):\n",
    "    source = [torch.LongTensor(item[0]) for item in batch ]#if item != [] and len(item) < 200 and len(item) > 4]\n",
    "    source = pad_sequence(source, batch_first=True, padding_value=0)\n",
    "    target = [list(item[1]) for item in batch ]#if item != [] and len(item) < 200 and len(item) > 4]\n",
    "    #decode target inputs and pad\n",
    "    target_dec_inputs = make_decoderinput(target)\n",
    "    target_dec_inputs = pad_sequence([torch.LongTensor(item) for item in target_dec_inputs], batch_first=True, padding_value=0)\n",
    "    #recover and decode target outputs and pad\n",
    "    target_dec_outputs = make_decoderoutput(recover_target(target))\n",
    "    target_dec_outputs = pad_sequence([torch.LongTensor(item) for item in target_dec_outputs], batch_first=True, padding_value=0)\n",
    "    return source, target_dec_inputs, target_dec_outputs\n",
    "    # #deault output size of 40\n",
    "    # train_enc_inputs =  [list(x) for x in list(zip(*batch))[0]]\n",
    "    # #remove any empty arrays\n",
    "    # train_enc_inputs = [x for x in train_enc_inputs if x != [] and len(x) < 101]\n",
    "    # train_enc_inputs = [convert_to_Longtensor(x) for x in train_enc_inputs]\n",
    "    # #remove any -1 values\n",
    "    # target_angles = [list(x) for x in list(zip(*batch))[1]]\n",
    "    # #remove any empty angles\n",
    "    # target_angles = [x for x in target_angles if x != [] and len(x) < 101]\n",
    "    # target_enc_inputs = make_decoderinput(target_angles)\n",
    "    # target_enc_inputs = [convert_to_Longtensor(x) for x in target_enc_inputs]\n",
    "    # #recover then decode for the target_dec\n",
    "    # target_dec_inputs = make_decoderinput(recover_target(target_angles))\n",
    "    # target_dec_inputs = [convert_to_Longtensor(x) for x in target_dec_inputs]\n",
    "    # return train_enc_inputs, target_enc_inputs, target_dec_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Transformer().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3, momentum=0.99)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()  # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    t_loss = 0.\n",
    "    start_time = time.time()\n",
    "    for step, (enc_inputs, dec_inputs, dec_outputs) in enumerate(train_loader):\n",
    "       # enc_inputs, dec_inputs, dec_outputs = enc_inputs.to(device), dec_inputs.to(device), dec_outputs.to(device)\n",
    "        outputs, enc_self_attns, dec_self_attns, dec_enc_attns = model(enc_inputs, dec_inputs)\n",
    "        loss = criterion(outputs, dec_outputs.view(-1))\n",
    "        # print(outputs,len(outputs),len(outputs[0]))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        t_loss += loss.item()\n",
    "        log_interval = 100\n",
    "        if step % log_interval == 0:\n",
    "            if( step != 0 ):\n",
    "                cur_loss = total_loss / log_interval\n",
    "                elapsed = (time.time() - start_time) / log_interval\n",
    "            else:\n",
    "                cur_loss = total_loss\n",
    "                elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | batches {:5d} | '\n",
    "                  'lr {:0.5f} | s/batch {:5.2f} | '\n",
    "                  'loss {:7.4f} '.format(epoch, step, lr_scheduler.get_last_lr()[0], elapsed , cur_loss))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "    return t_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angles = [\"psi_im1\", \"phi\", \"omega\", \"N_CA_C_angle\", \"C_N_CA_angle\", \"CA_C_N_angle\"]\n",
    "ss = [\"H\", \"E\", \"C\"]\n",
    "\n",
    "for secondary_structure in ss:\n",
    "        for angle in angles:\n",
    "            train_batchsize = 4\n",
    "            data = GetAngleForSSData(\"/kuhpc/work/slusky/s300y051/AnglesRefine/example/sample_source_targets\", secondary_structure, angle)\n",
    "            train_loader = DataLoader(data, batch_size = train_batchsize, shuffle = True, collate_fn = collate_fn_padded)\n",
    "            train_loss = train()\n",
    "            \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
